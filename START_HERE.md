# ğŸš€ NIC Public - Quick Start Guide

## âœ… YOUR SERVER IS NOW RUNNING!

The Flask app is live at: **http://localhost:5000**

---

## ğŸ¯ What's Working

âœ… **Vector Database**: 27 vehicle manual chunks loaded  
âœ… **FAISS Index**: Ready for semantic search  
âœ… **Flask Web Server**: Running on port 5000  
âœ… **Web UI**: Fully operational  
âœ… **LM Studio Ready**: Configured for local AI models  

---

## ğŸŒ Access the Application

### Option 1: Web Browser
1. Open your web browser
2. Go to: **http://localhost:5000**
3. Start asking vehicle maintenance questions!

### Option 2: Simple Browser (VS Code)
- The Simple Browser may already be open
- If not, use Ctrl+Shift+P â†’ "Simple Browser: Show"
- Navigate to `http://localhost:5000`

---

## ğŸ¤– Using with LM Studio (Offline AI)

### Step 1: Start LM Studio Server
1. Open **LM Studio** application
2. Go to **"Local Server"** tab (left sidebar)
3. Load your model:
   - `fireball-meta-llama-3.2-8b-instruct-agent-003-128k-code-dpo` (Recommended - Fast)
   - OR `qwen/qwen2.5-coder-14b` (Tier 2: deeper reasoning, ~5-10s per query)
4. Click **"Start Server"**
5. Verify it shows: `Server running on http://127.0.0.1:1234`

### Step 2: Test LM Studio Connection
```powershell
# In a new PowerShell window:
curl http://127.0.0.1:1234/v1/models
```

If working, you'll see JSON with your model info.

### Step 3: Use the App
- The Flask app **automatically detects** LM Studio
- No configuration needed!
- Just ask questions in the web UI

---

## ğŸ’¬ Example Queries to Try

### âœ… In-Scope (Will Work Well)
```
Engine cranks but won't start. What should I check?
What's the torque specification for lug nuts?
How do I test if my alternator is charging?
Battery voltage is low. What are the causes?
```

### âŒ Out-of-Scope (Will Refuse)
```
How do I rebuild a transmission?
What's the tire pressure for a 2024 Tesla?
```
(System will say "not in manual" - this is correct!)

---

## âš™ï¸ How It Works

### Without LM Studio (Retrieval Only)
- You get **raw context chunks** from the manual
- Fast, works offline
- No AI summarization

### With LM Studio (Full AI)
- Gets context chunks
- **AI summarizes** and explains
- **Citations** to manual sections
- More natural responses

---

## ğŸ›ï¸ UI Controls

### Safety Toggles (Top Right)
- **Citation Audit**: ON = Validates all claims against manual
- **Strict Mode**: ON = Direct quotes only (no paraphrasing)

### Model Selector (Input Area)
- **Auto**: Smart selection based on query
- **LLAMA (Fast)**: Quick responses, good for simple questions
- **GPT-OSS (Deep)**: Better for complex troubleshooting

*Both use your LM Studio models!*

---

## ğŸ”§ Troubleshooting

### "Server is running but page won't load"
```powershell
# Check if port 5000 is accessible
curl http://localhost:5000
```

### "LM Studio not connecting"
1. Verify LM Studio server is running (`http://127.0.0.1:1234`)
2. Check for firewall blocking port 1234
3. Try restarting LM Studio server

### "Slow responses"
- Use the **8B model** (fireball-llama-3.2) instead of 20B
- Enable **GPU offload** in LM Studio settings
- Reduce **Context Length** to 4096 in LM Studio

### "Out of memory"
- Use **quantized models** (Q4_K_M or Q5_K_M)
- Close other GPU-intensive applications
- Reduce batch size in LM Studio

---

## ğŸ“Š Performance Tips

### Speed Up Retrieval
```powershell
# Enable caching for 2000x speedup on repeat queries
$env:NOVA_ENABLE_RETRIEVAL_CACHE="1"
python nova_flask_app.py
```

### Optimize LM Studio
1. Settings â†’ **GPU Offload** â†’ Set to maximum (or adjust for VRAM)
2. Context Length â†’ **4096** (faster) or **8192** (better quality)
3. Use **Q4_K_M** quantized models for 3-4x speed boost

---

## ğŸ›‘ Stopping the Server

Press **Ctrl+C** in the terminal where Flask is running

---

## ğŸ“ Project Structure

```
C:\nova_rag_public\
â”œâ”€â”€ nova_flask_app.py          â† Main Flask server
â”œâ”€â”€ backend.py                  â† RAG logic, retrieval, LLM calls
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vehicle_manual.txt      â† Source manual (27 pages)
â”œâ”€â”€ vector_db/
â”‚   â”œâ”€â”€ vehicle_index.faiss     â† Vector database (27 vectors)
â”‚   â””â”€â”€ vehicle_docs.jsonl      â† Document metadata
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              â† Web UI
â””â”€â”€ static/
    â”œâ”€â”€ app.js                  â† Frontend logic
    â””â”€â”€ style.css               â† Styling
```

---

## ğŸ“ Next Steps

### Add More Documentation
1. Place PDF/TXT files in `data/`
2. Run: `python ingest_vehicle_manual.py`
3. Run: `python convert_index.py`
4. Restart Flask app

### Deploy to Production
```powershell
# Use waitress instead of Flask dev server
pip install waitress
waitress-serve --port=5000 nova_flask_app:app
```

### Share with Team
- Works on any machine with Python
- No API keys needed (uses LM Studio)
- Fully offline capable

---

## ğŸ“ Quick Reference

| What | Command |
|------|---------|
| **Start Server** | `python nova_flask_app.py` |
| **Access UI** | `http://localhost:5000` |
| **LM Studio URL** | `http://127.0.0.1:1234` |
| **Stop Server** | `Ctrl+C` in terminal |
| **Check Status** | `curl http://localhost:5000` |

---

## âœ¨ You're All Set!

Your NIC Public system is running and ready to answer vehicle maintenance questions!

ğŸŒ **Open**: http://localhost:5000  
ğŸ¤– **With AI**: Start LM Studio first  
ğŸ“– **Documentation**: See LM_STUDIO_SETUP.md for details  

**Happy troubleshooting!** ğŸš—ğŸ”§
